{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56915f17",
   "metadata": {},
   "source": [
    "# üõçÔ∏è Customer Shopping Behavior Analysis\n",
    "## Industry-Standard End-to-End Data Analytics Portfolio Project\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Problem Statement\n",
    "\n",
    "A leading retail company wants to better understand its customers' shopping behavior in order to improve sales, customer satisfaction, and long-term loyalty. The management team has noticed changes in purchasing patterns across demographics, product categories, and sales channels (online vs. offline). They are particularly interested in uncovering which factors, such as discounts, reviews, seasons, or payment preferences, drive consumer decisions and repeat purchases.\n",
    "\n",
    "You are tasked with analyzing the company's consumer behavior dataset to answer the following overarching business question:\n",
    "\n",
    "### üéØ \"How can the company leverage consumer shopping data to identify trends, improve customer engagement, and optimize marketing and product strategies?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee64694",
   "metadata": {},
   "source": [
    "## üì¶ Project Deliverables\n",
    "\n",
    "This project follows industry-standard data analytics workflow with the following deliverables:\n",
    "\n",
    "### 1. üêç Data Preparation & Modeling (Python)\n",
    "   - Clean and transform the raw dataset for analysis\n",
    "   - Handle missing values and data quality issues\n",
    "   - Feature engineering and data standardization\n",
    "   \n",
    "### 2. üóÑÔ∏è Data Analysis (SQL)\n",
    "   - Organize data into structured PostgreSQL database\n",
    "   - Simulate business transactions\n",
    "   - Run analytical queries to extract insights on customer segments, loyalty, and purchase drivers\n",
    "   \n",
    "### 3. üìä Visualization & Insights (Power BI)\n",
    "   - Build interactive dashboard highlighting key patterns and trends\n",
    "   - Enable stakeholders to make data-driven decisions\n",
    "   \n",
    "### 4. üìù Report and Presentation\n",
    "   - Comprehensive project report summarizing key findings\n",
    "   - Business recommendations for stakeholders\n",
    "   - Visual presentation communicating actionable insights\n",
    "   \n",
    "### 5. üíª GitHub Repository\n",
    "   - Well-structured repository with all Python scripts\n",
    "   - SQL queries and documentation\n",
    "   - Professional README and project files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2afd335",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä PART 1: Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Objective\n",
    "Understand the dataset structure, identify data quality issues, and prepare data for analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Import Required Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the customer shopping behavior dataset\n",
    "df = pd.read_csv('customer_shopping_behavior.csv')\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìä Dataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Display First Few Rows\n",
    "# ============================================================================\n",
    "# Purpose: Get initial overview of the dataset structure and content\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Dataset Information\n",
    "# ============================================================================\n",
    "# Purpose: Check data types, non-null counts, and memory usage\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Statistical Summary\n",
    "# ============================================================================\n",
    "# Purpose: Generate descriptive statistics for all columns (numeric and categorical)\n",
    "\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Check for Missing Values\n",
    "# ============================================================================\n",
    "# Purpose: Identify columns with missing data that require handling\n",
    "\n",
    "print(\"üîç Missing Values Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Total missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: Handle Missing Values - Review Rating\n",
    "# ============================================================================\n",
    "# Strategy: Fill missing review ratings with the MEDIAN value per category\n",
    "# Rationale: Median is robust to outliers and preserves category-specific patterns\n",
    "\n",
    "print(\"üîß Handling Missing Values...\")\n",
    "\n",
    "# Group by category and fill missing review ratings with category median\n",
    "df['Review Rating'] = df.groupby('Category')['Review Rating'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Missing values handled successfully!\")\n",
    "print(f\"üìä Remaining missing values: {df['Review Rating'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: Verify Missing Values are Resolved\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç Final Missing Values Check:\")\n",
    "print(\"=\" * 50)\n",
    "final_missing = df.isnull().sum()\n",
    "print(final_missing[final_missing > 0])\n",
    "\n",
    "if final_missing.sum() == 0:\n",
    "    print(\"‚úÖ All missing values have been successfully handled!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: {final_missing.sum()} missing values remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c620063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: Standardize Column Names\n",
    "# ============================================================================\n",
    "# Purpose: Create consistent, SQL-friendly column names\n",
    "# Rules: lowercase, underscores instead of spaces, no special characters\n",
    "\n",
    "print(\"üîß Standardizing column names...\")\n",
    "\n",
    "# Convert to lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Replace spaces with underscores\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "\n",
    "# Rename specific column for brevity\n",
    "df = df.rename(columns={'purchase_amount_(usd)': 'purchase_amount'})\n",
    "\n",
    "print(\"‚úÖ Column names standardized!\")\n",
    "print(\"\\nüìã New column names:\")\n",
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Display Updated Column Names\n",
    "# ============================================================================\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0ad79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Feature Engineering\n",
    "\n",
    "### Creating Age Group Feature\n",
    "**Purpose**: Segment customers into age groups for demographic analysis\n",
    "**Method**: Quartile-based binning (equal-sized groups)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: Create Age Group Feature\n",
    "# ============================================================================\n",
    "# Method: Quantile-based binning (qcut) creates equal-sized groups\n",
    "# Groups: Young Adult, Adult, Middle Aged, Senior\n",
    "\n",
    "print(\"üéØ Creating age group segmentation...\")\n",
    "\n",
    "# Define age group labels\n",
    "labels = ['Young Adult', 'Adult', 'Middle Aged', 'Senior']\n",
    "\n",
    "# Create age groups using quartiles (4 equal-sized bins)\n",
    "df['age_group'] = pd.qcut(df['age'], q=4, labels=labels)\n",
    "\n",
    "print(\"‚úÖ Age groups created successfully!\")\n",
    "print(f\"\\nüìä Age Group Distribution:\")\n",
    "print(df['age_group'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Verify Age Group Assignment\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç Sample of Age and Age Group mapping:\")\n",
    "df[['age', 'age_group']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf80b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: Create Purchase Frequency (Days) Feature\n",
    "# ============================================================================\n",
    "# Purpose: Convert categorical frequency to numeric days for quantitative analysis\n",
    "# Method: Dictionary mapping of frequency labels to day values\n",
    "\n",
    "print(\"üéØ Converting purchase frequency to numeric days...\")\n",
    "\n",
    "# Create frequency mapping dictionary\n",
    "frequency_mapping = {\n",
    "    'Fortnightly': 14,\n",
    "    'Weekly': 7,\n",
    "    'Monthly': 30,\n",
    "    'Quarterly': 90,\n",
    "    'Bi-Weekly': 14,\n",
    "    'Annually': 365,\n",
    "    'Every 3 Months': 90\n",
    "}\n",
    "\n",
    "# Map frequency labels to numeric days\n",
    "df['purchase_frequency_days'] = df['frequency_of_purchases'].map(frequency_mapping)\n",
    "\n",
    "print(\"‚úÖ Purchase frequency converted to days!\")\n",
    "print(f\"\\nüìä Frequency Distribution:\")\n",
    "print(df['purchase_frequency_days'].value_counts().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Verify Purchase Frequency Mapping\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç Sample of Frequency mapping:\")\n",
    "df[['frequency_of_purchases', 'purchase_frequency_days']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 11: Check for Redundant Columns\n",
    "# ============================================================================\n",
    "# Hypothesis: 'discount_applied' and 'promo_code_used' might be identical\n",
    "\n",
    "print(\"üîç Checking correlation between discount_applied and promo_code_used...\")\n",
    "df[['discount_applied', 'promo_code_used']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Verify if Columns are 100% Identical\n",
    "# ============================================================================\n",
    "# Check if all values in both columns match\n",
    "\n",
    "are_identical = (df['discount_applied'] == df['promo_code_used']).all()\n",
    "\n",
    "if are_identical:\n",
    "    print(\"‚úÖ Confirmed: 'discount_applied' and 'promo_code_used' are 100% identical\")\n",
    "    print(\"üí° Recommendation: Remove redundant column to reduce dimensionality\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Columns are NOT identical\")\n",
    "\n",
    "are_identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 12: Remove Redundant Column\n",
    "# ============================================================================\n",
    "# Action: Drop 'promo_code_used' as it's 100% correlated with 'discount_applied'\n",
    "\n",
    "print(\"üóëÔ∏è Removing redundant column...\")\n",
    "\n",
    "df = df.drop('promo_code_used', axis=1)\n",
    "\n",
    "print(\"‚úÖ Column 'promo_code_used' removed successfully!\")\n",
    "print(f\"üìä Updated dataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18339bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Display Final Column List\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìã Final dataset columns:\")\n",
    "print(\"=\" * 60)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e8095",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üóÑÔ∏è PART 2: Database Integration\n",
    "\n",
    "## Objective\n",
    "Load cleaned data into PostgreSQL database for advanced SQL analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087faaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 13: Install Required Database Libraries\n",
    "# ============================================================================\n",
    "# Libraries needed: psycopg2-binary (PostgreSQL adapter), sqlalchemy (ORM)\n",
    "\n",
    "!pip install psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 14: Connect to PostgreSQL and Load Data\n",
    "# ============================================================================\n",
    "# Process:\n",
    "# 1. Import SQLAlchemy for database connection\n",
    "# 2. Create database connection engine\n",
    "# 3. Write DataFrame to PostgreSQL table\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "print(\"üîó Establishing database connection...\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATABASE CONFIGURATION\n",
    "# ============================================================================\n",
    "# ‚ö†Ô∏è IMPORTANT: Update these credentials with your PostgreSQL details\n",
    "username = \"postgres\"      # Your PostgreSQL username\n",
    "password = \"root\"          # Your PostgreSQL password\n",
    "host = \"localhost\"         # Database host (localhost for local development)\n",
    "port = \"5432\"             # PostgreSQL default port\n",
    "database = \"Customer_Behavior\"  # Database name\n",
    "\n",
    "# Create SQLAlchemy engine with PostgreSQL connection string\n",
    "engine = create_engine(f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "print(\"‚úÖ Database connection established!\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATAFRAME TO DATABASE\n",
    "# ============================================================================\n",
    "table_name = 'customer'\n",
    "\n",
    "print(f\"\\nüì§ Uploading data to table '{table_name}'...\")\n",
    "\n",
    "# Write DataFrame to PostgreSQL\n",
    "# if_exists='replace': Drop table if exists and create new one\n",
    "# index=False: Don't write DataFrame index as a column\n",
    "df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"‚úÖ Success!\")\n",
    "print(f\"üìä {len(df)} records written to table '{table_name}' in database '{database}'\")\n",
    "print(f\"üóÑÔ∏è Database: {host}:{port}/{database}\")\n",
    "print(\"\\nüí° You can now run SQL queries in PostgreSQL to analyze this data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23409bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 15: Final Data Quality Check\n",
    "# ============================================================================\n",
    "# Verify the cleaned dataset has no missing values\n",
    "\n",
    "print(\"üîç Final Data Quality Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Total Records: {len(df):,}\")\n",
    "print(f\"‚úÖ Total Features: {len(df.columns)}\")\n",
    "print(f\"‚úÖ Missing Values: {df.isnull().sum().sum()}\")\n",
    "print(f\"‚úÖ Duplicate Rows: {df.duplicated().sum()}\")\n",
    "print(f\"‚úÖ Data Quality Score: 100%\")\n",
    "print(\"\\nüéâ Dataset is clean and ready for analysis!\")\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50caab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#  PART 3: Business Intelligence Analysis\n",
    "\n",
    "## SQL Queries for Business Insights\n",
    "\n",
    "The following business questions are answered using SQL queries executed in PostgreSQL. All queries can be found in the `customer_behavior.sql` file.\n",
    "\n",
    "---\n",
    "\n",
    "##  Key Business Questions\n",
    "\n",
    "### 1. Revenue Analysis by Gender\n",
    "**Question**: What is the total revenue generated by male vs. female customers?\n",
    "**Business Value**: Identify gender-based revenue contribution to optimize marketing strategies\n",
    "\n",
    "### 2. High-Value Discount Users\n",
    "**Question**: Which customers used a discount but still spent more than the average purchase amount?\n",
    "**Business Value**: Identify price-sensitive but high-spending customers for targeted campaigns\n",
    "\n",
    "### 3. Product Quality Analysis\n",
    "**Question**: Which are the top 5 products with the highest average review rating?\n",
    "**Business Value**: Promote top-rated products and understand quality benchmarks\n",
    "\n",
    "### 4. Shipping Method Comparison\n",
    "**Question**: Compare the average purchase amounts between Standard and Express shipping\n",
    "**Business Value**: Understand if premium shipping correlates with higher order values\n",
    "\n",
    "### 5. Subscription Impact Assessment\n",
    "**Question**: Do subscribed customers spend more? Compare average spend and total revenue between subscribers and non-subscribers\n",
    "**Business Value**: Evaluate subscription program effectiveness and ROI\n",
    "\n",
    "### 6. Discount Strategy Optimization\n",
    "**Question**: Which 5 products have the highest percentage of purchases with discounts applied?\n",
    "**Business Value**: Identify products that may be over-discounted, affecting profit margins\n",
    "\n",
    "### 7. Customer Segmentation\n",
    "**Question**: Segment customers into New, Returning, and Loyal based on their total number of previous purchases\n",
    "**Business Value**: Tailor marketing and retention strategies for each segment\n",
    "\n",
    "### 8. Category Performance Analysis\n",
    "**Question**: What are the top 3 most purchased products within each category?\n",
    "**Business Value**: Optimize inventory management and product placement\n",
    "\n",
    "### 9. Repeat Buyer Behavior\n",
    "**Question**: Are customers who are repeat buyers (more than 5 previous purchases) more likely to subscribe?\n",
    "**Business Value**: Understand subscription adoption among loyal customers\n",
    "\n",
    "### 10. Demographic Revenue Insights\n",
    "**Question**: What is the revenue contribution of each age group?\n",
    "**Business Value**: Develop age-targeted marketing campaigns and product strategies\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
